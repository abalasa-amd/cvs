"""
Parser for Aorta TraceLens analysis reports.

Parses the Excel reports generated by Aorta's built-in analysis scripts:
- scripts/tracelens_single_config/run_tracelens_single_config.sh
- scripts/tracelens_single_config/process_gpu_timeline.py

This replaces direct TraceLens parsing in CVS - we now leverage Aorta's
more sophisticated analysis pipeline which includes GEMM patching and
comprehensive report generation.

Copyright 2025 Advanced Micro Devices, Inc.
All rights reserved.
"""

import logging
from pathlib import Path
from typing import Any, Dict, List, Optional

from parsers.schemas import (
    AortaTraceMetrics,
    AortaBenchmarkResult,
    ParseResult,
    ParseStatus,
)
from runners._base_runner import RunResult

log = logging.getLogger(__name__)

# Try to import pandas/openpyxl for Excel parsing
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    pd = None  # type: ignore


class AortaReportParser:
    """
    Parser for Aorta's TraceLens analysis Excel reports.
    
    Reads the Excel reports generated by Aorta's built-in analysis scripts
    rather than parsing raw traces directly. This provides:
    
    1. More accurate metrics (Aorta's GEMM-patched TraceLens)
    2. Pre-computed aggregations (gpu_timeline_summary_mean.xlsx)
    3. Collective multi-rank analysis
    4. Consistent report format
    
    Expected directory structure (created by Aorta's scripts):
    
        output_dir/
        ├── torch_profiler/
        │   ├── rank0/trace.json
        │   └── ...
        └── tracelens_analysis/
            ├── individual_reports/
            │   ├── perf_rank0.xlsx
            │   ├── perf_rank1.xlsx
            │   └── ...
            ├── collective_reports/
            │   └── collective_all_ranks.xlsx
            └── gpu_timeline_summary_mean.xlsx
    """
    
    def __init__(self):
        """Initialize parser."""
        if not PANDAS_AVAILABLE:
            raise ImportError(
                "pandas is required for AortaReportParser. "
                "Install with: pip install pandas openpyxl"
            )
    
    def parse(self, run_result: RunResult) -> ParseResult[AortaTraceMetrics]:
        """
        Parse benchmark results from Aorta's analysis reports.
        
        Args:
            run_result: Result from AortaRunner (must include tracelens_analysis artifact)
            
        Returns:
            ParseResult containing validated AortaTraceMetrics for each rank
        """
        if not run_result.succeeded:
            return ParseResult(
                status=ParseStatus.FAILED,
                errors=[f"Run did not succeed: {run_result.error_message}"]
            )
        
        # First try the tracelens_analysis artifact
        analysis_dir = run_result.get_artifact("tracelens_analysis")
        if analysis_dir and analysis_dir.exists():
            return self.parse_analysis_directory(analysis_dir)
        
        # Fallback: look for analysis dir relative to trace dir
        trace_dir = run_result.get_artifact("torch_traces")
        if trace_dir:
            parent_dir = trace_dir.parent
            analysis_dir = parent_dir / "tracelens_analysis"
            if analysis_dir.exists():
                return self.parse_analysis_directory(analysis_dir)
        
        return ParseResult(
            status=ParseStatus.FAILED,
            errors=[
                "No tracelens_analysis artifact found. "
                "Ensure analysis.enable_tracelens is set in config."
            ]
        )
    
    def parse_analysis_directory(self, analysis_dir: Path) -> ParseResult[AortaTraceMetrics]:
        """
        Parse all reports in a tracelens_analysis directory.
        
        Args:
            analysis_dir: Path to tracelens_analysis/ directory
            
        Returns:
            ParseResult with metrics for each rank
        """
        results: List[AortaTraceMetrics] = []
        warnings: List[str] = []
        errors: List[str] = []
        
        individual_dir = analysis_dir / "individual_reports"
        
        if not individual_dir.exists():
            return ParseResult(
                status=ParseStatus.FAILED,
                errors=[f"Individual reports not found: {individual_dir}"]
            )
        
        # Find all per-rank reports
        report_files = sorted(individual_dir.glob("perf_rank*.xlsx"))
        
        if not report_files:
            # Try alternative naming pattern (with channel info)
            report_files = sorted(individual_dir.glob("perf_*ch_rank*.xlsx"))
        
        if not report_files:
            return ParseResult(
                status=ParseStatus.FAILED,
                errors=[f"No perf_rank*.xlsx files found in {individual_dir}"]
            )
        
        log.info(f"Found {len(report_files)} individual reports to parse")
        
        for report_file in report_files:
            try:
                metrics = self._parse_individual_report(report_file)
                results.append(metrics)
                log.debug(f"Parsed rank {metrics.rank}: {metrics.total_time_us:.2f}us")
            except Exception as e:
                warnings.append(f"Failed to parse {report_file.name}: {e}")
                log.warning(f"Failed to parse {report_file.name}: {e}")
        
        # Determine status
        if not results:
            status = ParseStatus.FAILED
            errors.append("No reports could be parsed")
        elif warnings:
            status = ParseStatus.PARTIAL
        else:
            status = ParseStatus.SUCCESS
        
        return ParseResult(
            status=status,
            results=results,
            warnings=warnings,
            errors=errors,
            metadata={
                "analysis_dir": str(analysis_dir),
                "reports_found": len(report_files),
                "reports_parsed": len(results),
            }
        )
    
    def _parse_individual_report(self, report_file: Path) -> AortaTraceMetrics:
        """
        Parse a single per-rank TraceLens report.
        
        Args:
            report_file: Path to perf_rank*.xlsx file
            
        Returns:
            AortaTraceMetrics for this rank
        """
        # Extract rank from filename
        rank = self._extract_rank_from_filename(report_file.name)
        
        # Read the gpu_timeline sheet
        try:
            df = pd.read_excel(report_file, sheet_name="gpu_timeline")
        except ValueError:
            # Sheet not found, try alternative sheet names
            xl = pd.ExcelFile(report_file)
            if "GPU Timeline" in xl.sheet_names:
                df = pd.read_excel(report_file, sheet_name="GPU Timeline")
            else:
                raise ValueError(f"No gpu_timeline sheet found in {report_file}")
        
        # Create lookup from type -> time ms
        # TraceLens output has 'type' and 'time ms' columns
        timeline = dict(zip(df['type'], df['time ms']))
        
        # Extract metrics (TraceLens outputs in ms, convert to us)
        total_time_ms = timeline.get('total_time', 0)
        compute_time_ms = timeline.get('computation_time', 0)
        exposed_comm_ms = timeline.get('exposed_comm_time', 0)
        total_comm_ms = timeline.get('total_comm_time', 0)
        
        # Convert to microseconds
        total_time_us = total_time_ms * 1000
        compute_time_us = compute_time_ms * 1000
        # Use exposed comm (non-overlapped) for the communication cost metric
        comm_time_us = exposed_comm_ms * 1000
        
        log.debug(f"Rank {rank}: total={total_time_ms:.1f}ms, "
                  f"compute={compute_time_ms:.1f}ms, "
                  f"exposed_comm={exposed_comm_ms:.1f}ms")
        
        return AortaTraceMetrics(
            rank=rank,
            total_time_us=float(total_time_us),
            compute_time_us=float(compute_time_us),
            communication_time_us=float(comm_time_us),
        )
    
    def _extract_rank_from_filename(self, filename: str) -> int:
        """
        Extract rank number from report filename.
        
        Handles patterns like:
        - perf_rank0.xlsx -> 0
        - perf_rank7.xlsx -> 7
        - perf_112ch_rank0.xlsx -> 0
        """
        import re
        
        # Try to find rank number
        match = re.search(r'rank(\d+)', filename)
        if match:
            return int(match.group(1))
        
        log.warning(f"Could not extract rank from {filename}, defaulting to 0")
        return 0
    
    def parse_summary(self, analysis_dir: Path) -> Optional[Dict[str, float]]:
        """
        Parse the aggregated summary report if available.
        
        Args:
            analysis_dir: Path to tracelens_analysis/ directory
            
        Returns:
            Dictionary with aggregated metrics, or None if not found
        """
        summary_file = analysis_dir / "gpu_timeline_summary_mean.xlsx"
        
        if not summary_file.exists():
            log.debug(f"Summary file not found: {summary_file}")
            return None
        
        try:
            df = pd.read_excel(summary_file, sheet_name="Summary")
            
            # Create lookup from type -> aggregated values
            timeline = dict(zip(df['type'], df['time ms']))
            
            return {
                "total_time_ms": timeline.get('total_time', 0),
                "computation_time_ms": timeline.get('computation_time', 0),
                "exposed_comm_time_ms": timeline.get('exposed_comm_time', 0),
                "total_comm_time_ms": timeline.get('total_comm_time', 0),
                "num_ranks": int(df['num_ranks'].iloc[0]) if 'num_ranks' in df.columns else 0,
            }
        except Exception as e:
            log.warning(f"Failed to parse summary: {e}")
            return None
    
    def aggregate(
        self,
        parse_result: ParseResult[AortaTraceMetrics],
        num_nodes: int,
        gpus_per_node: int,
        **metadata
    ) -> Optional[AortaBenchmarkResult]:
        """
        Aggregate per-rank metrics into a benchmark result.
        
        Args:
            parse_result: Parsed per-rank metrics
            num_nodes: Number of nodes in cluster
            gpus_per_node: GPUs per node
            **metadata: Additional metadata fields
            
        Returns:
            Aggregated benchmark result, or None if aggregation failed
        """
        if not parse_result.has_results:
            log.error("Cannot aggregate: no results to aggregate")
            return None
        
        try:
            return AortaBenchmarkResult.from_rank_metrics(
                metrics=parse_result.results,
                num_nodes=num_nodes,
                gpus_per_node=gpus_per_node,
                **metadata
            )
        except Exception as e:
            log.exception(f"Aggregation failed: {e}")
            return None
    
    def validate_thresholds(
        self,
        result: AortaBenchmarkResult,
        expected: Dict[str, Any]
    ) -> List[str]:
        """
        Validate benchmark results against expected thresholds.
        
        Args:
            result: Aggregated benchmark result
            expected: Dictionary of threshold configurations
            
        Returns:
            List of validation failure messages (empty if all pass)
        """
        failures = []
        
        # Check iteration time
        max_iteration_ms = expected.get("max_avg_iteration_ms")
        if max_iteration_ms is not None:
            if result.avg_iteration_time_ms > max_iteration_ms:
                failures.append(
                    f"Average iteration time {result.avg_iteration_time_ms:.2f}ms "
                    f"exceeds threshold {max_iteration_ms}ms"
                )
        
        # Check compute ratio
        min_compute_ratio = expected.get("min_compute_ratio")
        if min_compute_ratio is not None:
            if result.avg_compute_ratio < min_compute_ratio:
                failures.append(
                    f"Average compute ratio {result.avg_compute_ratio:.2%} "
                    f"below threshold {min_compute_ratio:.2%}"
                )
        
        # Check overlap ratio
        min_overlap_ratio = expected.get("min_overlap_ratio")
        if min_overlap_ratio is not None:
            if result.avg_overlap_ratio < min_overlap_ratio:
                failures.append(
                    f"Average overlap ratio {result.avg_overlap_ratio:.2%} "
                    f"below threshold {min_overlap_ratio:.2%}"
                )
        
        # Check per-rank variance (iteration time skew)
        max_time_variance = expected.get("max_time_variance_ratio")
        if max_time_variance is not None:
            if result.avg_iteration_time_us > 0:
                variance_ratio = result.std_iteration_time_us / result.avg_iteration_time_us
                if variance_ratio > max_time_variance:
                    failures.append(
                        f"Iteration time variance {variance_ratio:.2%} "
                        f"exceeds threshold {max_time_variance:.2%}"
                    )
        
        return failures

